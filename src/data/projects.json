[
  {
    "title": "Word - Mate",
    "description": "An escape from the real world. Built for logophiles. Just follow the link, enter your name and hit Start Game to create or join a game room. If you don't get paired, then probably there are no other players online at the moment. Login from a different device or share the link with your friends to play together!",
    "TechStack": [
      "Node,js",
      " Next.js",
      "Websockets",
      "Tailwind CSS",
      "Express.js"
    ],
    "url": "https://wordmate-nu.vercel.app/",
    "tags": ["Full-Stack Web Development"]
  },
  {
    "title": "Personal Portfolio Website",
    "description": "A personal portfolio website built using Next.js, Tailwind CSS and TypeScript. It showcases my projects, skills and experience in a clean and responsive design.",
    "tags": ["Frontend Web Development"],
    "TechStack": ["Next.js", "Tailwind CSS", "TypeScript"],
    "url": "https://ayushaman.dev"
  },
  {
    "title": "LLM generated Behaviour Tree for robotic mission planning",

    "description": "Investigated whether Large Language Models (LLMs) can automatically generate Behaviour Trees (BTs) for complex autonomous robotic missions, specifically focusing on bridge inspection using multiple UAVs. The research compared three approaches: manual human creation, fully automated LLM generation, and human-in-the-loop assisted generation.\n\nProcess\n-------\n\n### Mission Context\n\nDesigned a simulated bridge inspection mission where two UAVs coordinate to deploy structural health monitoring sensors. UAV-1 prepares the surface by cleaning and applying adhesive, then signals UAV-2 to retrieve and attach the sensor at the prepared location. This workflow mirrors real-world bridge inspection procedures requiring precise multi-robot coordination.\n\n### Control Architecture Selection\n\nChose Behaviour Trees over traditional Finite State Machines due to their superior modularity, scalability, and reactivity. BTs organize robot behaviours hierarchically with control-flow nodes (Sequential, Fallback, Parallel) directing execution to action nodes, making complex missions more maintainable and adaptable to environmental changes.\n\n### Simulation Environment\n\nBuilt the experimental testbed using:\n\n*   **ROS 2 Humble** for robot communication and coordination\n    \n*   **Gazebo Ignition** for realistic 3D physics simulation\n    \n*   **Aerostack 2** providing pre-built UAV control primitives\n    \n*   **py_trees** library for Behaviour Tree construction and execution\n    \n\n### Three Generation Methods\n\n**Manual Construction:**A human expert designed the complete Behaviour Tree from scratch, implementing proper sequencing, fallback recovery mechanisms, and parallel execution for both UAVs. This established the performance baseline but required approximately one month including learning curve and debugging.\n\n**LLM-Only Generation:**Prompted multiple LLMs (ChatGPT-5, DeepSeek 3.1, Claude Sonnet 4) with the Standard Operating Procedure and available behaviours. Models were instructed to generate executable Behaviour Trees using the latest library versions and proper control flow structures.\n\n**Human-in-the-Loop (HITL):**Started with LLM-generated trees and applied minimal human corrections focused only on execution-blocking errors—not restructuring logic or improving design. This hybrid approach aimed to combine LLM speed with human debugging capability.\n\n### Evaluation Framework\n\nAssessed generated Behaviour Trees across multiple dimensions:\n\n*   **Success Rate:** Mission completion percentage across multiple runs\n    \n*   **Logical Coherence:** Proper sequencing and structure of behaviours\n    \n*   **Executability:** Whether code runs without syntax/runtime errors\n    \n*   **Computational Complexity:** Growth order with increasing waypoints\n    \n*   **Execution Time:** Average duration to complete the mission\n    \nResults\n-------\n\n### Manual Method Performance\n\nAchieved **90% success rate** with one failure due to mid-mission drone collision. Average execution time was 87.4 seconds for successful runs. The tree demonstrated high logical coherence with proper fallback mechanisms, though it lacked some optimizations like decorator nodes for waypoint looping.\n\n### LLM-Only Limitations\n\nAll three models **failed completely** (0% success rate):\n\n*   **ChatGPT-5:** Generated code using outdated py_trees syntax initially; subsequent attempts produced trees that got stuck on individual behaviours\n    \n*   **DeepSeek 3.1:** Created trees that halted during the arming phase due to repeated pre-mission procedures\n    \n*   **Claude Sonnet 4:** Best attempt completed the first waypoint but incorrectly repeated takeoff/arming sequences for subsequent waypoints, causing system errors\n    \nCommon issues included outdated library dependencies, semantically incorrect behaviour sequences, and missing compatibility with Aerostack 2 requirements.\n\n### Human-in-the-Loop Success\n\nAchieved **100% success rate** with average execution time of 83.6 seconds—the fastest of all methods:\n\n*   **ChatGPT-5 + HITL:** Required 4 modifications over 1,005 seconds debugging time\n    \n*   **DeepSeek 3.1 + HITL:** Needed 3 modifications over 775 seconds\n    \n*   **Claude Sonnet 4 + HITL:** Only 1 modification in just 240 seconds—primarily moving pre-mission procedures outside waypoint loops\n    \nThe corrected trees maintained medium logical coherence (some semantic quirks remained) but achieved full executability and performed comparably to or better than the manual baseline.\n\n### Comparative Analysis\n\nAll methods produced O(n) computational complexity, though decorator-based looping could achieve O(1). The HITL approach emerged as optimal, combining LLM generation speed with human error correction, producing working trees in minutes rather than weeks while matching or exceeding manual baseline performance.\n\nKey Findings\n------------\n\n**LLM Capabilities:** Current models can generate syntactically valid Behaviour Tree structures and understand high-level mission requirements, but struggle with domain-specific constraints, library version compatibility, and maintaining logical coherence across complex multi-step sequences.\n\n**Human-in-the-Loop Value:** Minimal human intervention (correcting 1-4 critical errors) transformed non-functional LLM outputs into high-performing solutions, suggesting a practical path forward for rapid mission planning.\n\n**Practical Implications:** For safety-critical applications like bridge inspection, fully autonomous LLM generation remains insufficient, but supervised generation with expert validation can significantly accelerate development while maintaining reliability.\n\nTechnical Significance\n----------------------\n\nThis research demonstrates that Behaviour Trees combined with LLM-assisted generation offer a promising pathway toward reducing design effort in autonomous robotics. While full autonomy in mission planning remains elusive, the hybrid approach provides immediate practical value for complex multi-robot coordination tasks beyond bridge inspection, applicable to any scenario requiring modular, verifiable, and adaptive autonomous systems.",
    "TechStack": [
      "ROS 2 Humble",
      "Gazebo Ignition",
      "Aerostack 2",
      "py_trees",
      "Python",
      "ChatGPT-5",
      "DeepSeek 3.1",
      "Claude Sonnet 4"
    ],
    "tags": [
      "Machine Learning",
      "GenAI",
      "Robotics",
      "Behavior Trees",
      "LLMs",
      "Autonomous Systems",
      "Robotic Mission Planning",
      "Research"
    ],
    "url": "https://drive.google.com/file/d/1z53IXTxKQRpQK6YuVse6kwjzxgrRHqKl/view?usp=sharing"
  },
  {
    "title": "Traffic Signal Classifier",

    "description": "Developed a deep learning model to automatically classify traffic signals into 43 different categories using computer vision. The system employs a LeNet-inspired Convolutional Neural Network architecture to recognize various traffic signs from images, which is crucial for autonomous vehicle navigation and driver assistance systems.\n\nProcess\n-------\n\n### Data Preparation\n\nStarted with a pre-split dataset containing training, validation, and test sets of traffic signal images. Each image was 32\u00d732 pixels in RGB format. Initial exploration revealed the dataset structure and verified proper loading of all three subsets with their corresponding labels.\n\n### Preprocessing Pipeline\n\nApplied a comprehensive image preprocessing workflow to optimize model performance:\n\n*   **Data Shuffling:** Randomized the training set to prevent learning order bias\n    \n*   **Grayscale Conversion:** Reduced RGB images to single-channel grayscale by averaging color channels, simplifying the input while retaining essential shape information\n    \n*   **Normalization:** Applied mean centering and scaling to range [-1, 1] using the formula (pixel - 128)/128, helping the network converge faster during training\n    \n\n### Model Architecture\n\nImplemented a LeNet-based Convolutional Neural Network optimized for traffic sign recognition:\n\n*   **First Convolutional Block:** 6 filters with 5\u00d75 kernels and ReLU activation, followed by average pooling\n    \n*   **Second Convolutional Block:** 16 filters with 5\u00d75 kernels and ReLU activation, followed by average pooling\n    \n*   **Flattening Layer:** Converted 2D feature maps to 1D vector\n    \n*   **Fully Connected Layers:** Three dense layers with 120, 84, and 43 units respectively\n    \n*   **Output Layer:** 43-class classification using sigmoid activation\n    \n*   **Optimization:** Sparse categorical cross-entropy loss with Adam optimizer (learning rate 0.001)\n    \n\n### Training Strategy\n\nTrained the model over 50 epochs with batch size of 500, monitoring both training and validation performance to track learning progress and detect potential overfitting. The validation set provided real-time feedback on generalization capability.\n\n### Performance Evaluation\n\nAssessed the trained model using multiple complementary approaches:\n\n*   Quantitative metrics on the held-out test set\n    \n*   Training/validation accuracy and loss curves across epochs\n    \n*   Confusion matrix visualizing classification patterns across all 43 classes\n    \n*   Visual inspection of predictions on sample test images\n    \nResults\n-------\n\n### Model Performance\n\nThe traffic signal classifier achieved **strong accuracy** on the test set, successfully distinguishing between the 43 different traffic sign categories.\n\n**Training Dynamics:**\n\n*   Both training and validation accuracy showed steady improvement across the 50 epochs\n    \n*   The learning curves indicated healthy convergence without significant overfitting\n    \n*   Loss decreased consistently for both training and validation sets, demonstrating effective learning\n    \n\n### Classification Analysis\n\n**Confusion Matrix Insights:**The detailed 43\u00d743 confusion matrix revealed the model's prediction patterns across all sign categories. Strong diagonal values indicated correct classifications, while off-diagonal elements showed which sign types were occasionally confused with each other.\n\n**Visual Prediction Assessment:**A 7\u00d77 grid of test images with predicted and true labels provided qualitative validation. This visualization allowed direct inspection of where the model succeeded and which sign types posed challenges, offering insights into potential areas for improvement.\n\n### Key Strengths\n\nThe model demonstrated robust performance in distinguishing between visually similar traffic signs, successfully learning discriminative features from the limited 32\u00d732 pixel resolution. The grayscale preprocessing proved effective, suggesting that shape and intensity patterns\u2014rather than color\u2014were sufficient for accurate classification in this domain.\n\nTechnical Approach\n------------------\n\nThe project successfully adapted the classic LeNet architecture for modern traffic sign recognition. By converting to grayscale and normalizing inputs, the model achieved efficient training while maintaining strong classification accuracy. The convolutional layers effectively extracted hierarchical features from sign shapes, while the fully connected layers mapped these features to the 43 sign categories.\n\nThis system demonstrates practical applicability for real-world autonomous driving systems and driver assistance technologies, where rapid and accurate traffic sign recognition is essential for safe navigation.",

    "TechStack": [
      "Tensorflow",
      "Keras",
      "scikit learn",
      "numpy",
      "pandas",
      "matplotlib",
      "seaborn"
    ],
    "url": "https://colab.research.google.com/drive/1NAmVJFPmKZo1iEr9bM8bsm_qOpcLNv15",
    "tags": ["Machine Learning", "Deep Learning"]
  },

  {
    "title": "Spam Classifier",

    "description": "Built a machine learning classifier to automatically detect spam messages using natural language processing and Naive Bayes algorithm. The system analyzes text patterns to distinguish between legitimate messages (ham) and spam.\n\nProcess\n-------\n\n### Data Preparation\n\nThe project began with loading and exploring a dataset of labeled messages. Initial analysis examined the distribution between spam and ham messages to understand class balance and dataset characteristics.\n\n### Text Preprocessing\n\nApplied a comprehensive text cleaning pipeline to standardize the message data:\n\n*   **Noise Removal**: Stripped special characters and punctuation, retaining only alphabetic characters\n    \n*   **Normalization**: Converted all text to lowercase for consistency\n    \n*   **Stop Word Removal**: Eliminated common English words that don't contribute to spam detection\n    \n*   **Stemming**: Reduced words to their root forms using Porter Stemmer algorithm to capture semantic meaning\n    \n\n### Feature Engineering\n\nTransformed the cleaned text into numerical features using the Bag-of-Words approach with CountVectorizer. This created a matrix representation where each unique word became a feature, and values represented word frequencies in each message.\n\n### Model Training\n\nSplit the dataset into training and testing sets, then trained a Multinomial Naive Bayes classifier. This algorithm is particularly well-suited for text classification tasks as it works effectively with discrete word count features and performs well even with limited training data.\n\n### Model Evaluation\n\nTested the trained model on unseen data to assess its real-world performance using multiple metrics.\n\nResults\n-------\n\nThe spam classifier achieved strong performance with **high accuracy** in distinguishing spam from legitimate messages.\n\n**Key Findings:**\n\n*   The model demonstrated robust precision and recall across both classes\n    \n*   Confusion matrix analysis revealed the classifier's ability to minimize both false positives (legitimate messages marked as spam) and false negatives (spam messages marked as legitimate)\n    \n*   The classification report provided detailed performance metrics showing balanced performance on both spam and ham detection\n    \n\n**Performance Visualization:** A heatmap of the confusion matrix clearly illustrated the model's prediction patterns, showing strong diagonal values indicating correct classifications and minimal off-diagonal misclassifications.\n\nTechnical Approach\n------------------\n\nThe project successfully combined traditional NLP techniques with machine learning to create an effective spam detection system. The Multinomial Naive Bayes algorithm proved to be an efficient choice, offering both good performance and computational efficiency suitable for real-time message classification.",

    "TechStack": [
      "Tensorflow",
      "Keras",
      "scikit learn",
      "numpy",
      "pandas",
      "matplotlib",
      "seaborn"
    ],
    "tags": ["Machine Learning", "Deep Learning"],
    "url": "https://colab.research.google.com/drive/14wyNYvSV2dXc6np2xxDET8-jyNASZLuv?usp=sharing"
  },
  {
    "title": "Astronomical Object Analysis using Computer Vision",

    "description": "Developed a comprehensive computer vision system to analyze a rotating spherical Astronomical Object (AO)\u2014a large globe hanging in an atrium\u2014to assist in a theoretical drone landing scenario. The project implemented multiple segmentation techniques, geometric measurements, and rotation tracking algorithms using real-time video processing to extract critical information about the object's position, movement, and rotational dynamics.\n\nProcess\n-------\n\n### Equipment and Tools\n\nUsed an Arducam OV9782 RGB camera to capture images and videos of the AO. The implementation leveraged OpenCV for image processing and computer vision operations, NumPy for mathematical computations, Matplotlib for visualization, and scikit-learn for performance evaluation. A dataset of 100 images with ground-truth masks enabled quantitative assessment of segmentation accuracy.\n\n### Task 1: Object Segmentation\n\n**Dual-Method Approach:**Implemented two complementary segmentation strategies to isolate the AO from background elements.\n\n**Color Thresholding Method:**\n\n*   Converted images from RGB to HSV color space to decouple color information from brightness and intensity\n    \n*   Defined multiple HSV ranges targeting the ocean's blues, cloud whites, and landmass colors on the globe\n    \n*   Created binary masks using cv2.inRange() for each color range\n    \n*   Combined masks using bitwise OR operations to capture all relevant colors\n    \n*   Applied morphological closing and opening operations with elliptical kernels to remove noise and fill gaps\n    \n*   Final mask isolated the AO based on its distinctive color signature\n    \n\n**Edge Detection Method:**\n\n*   Converted images to grayscale to reduce computational complexity while preserving edge information\n    \n*   Applied Gaussian blur to reduce noise and smooth minor details\n    \n*   Used Hough Circle Transform (cv2.HoughCircles()) to detect the spherical AO geometry\n    \n*   Tuned parameters (dp=1.1, param1=50, param2=30) to reliably detect circles across the dataset\n    \n*   Created circular mask by drawing the detected circle on an empty mask\n    \n*   Isolated the AO based purely on geometric shape rather than color\n    \n\n**Combined Method for Enhanced Accuracy:**Merged both approaches to leverage their complementary strengths\u2014color-based precision for region identification and geometry-based accuracy for boundary definition. Applied bitwise AND operation between the color mask and circle mask, ensuring only pixels satisfying both color and geometric criteria were included. This hybrid approach significantly reduced false positives and improved overall segmentation quality.\n\n**Performance Evaluation:**Generated ROC (Receiver Operating Characteristic) curves comparing all three methods against ground-truth masks. Calculated true positive rates versus false positive rates across the 100-image dataset to quantitatively assess each method's classification performance. Visualized results as scatter plots with random chance baseline for comparison.\n\n### Task 2: Geometric Measurements\n\n**Centroid Detection:**Extracted contours from the segmented binary mask using cv2.findContours(), selected the largest contour corresponding to the AO, and computed spatial moments using cv2.moments(). Calculated centroid coordinates using the formulas: Centroid_x = M[m10]/M[m00] and Centroid_y = M[m01]/M[m00]. Marked the centroid on original images with colored circles for visual verification.\n\n**Movement Tracking Over Time:**Captured image sequences from a static camera position to track centroid displacement. Applied the segmentation and centroid detection pipeline to each frame, storing pixel coordinates in arrays. Plotted the centroid trajectory as X-Y coordinates over time to visualize and quantify the AO's swinging motion patterns.\n\n**Height Estimation:**Calibrated the camera using checkerboard patterns to obtain intrinsic parameters (focal length fx=900.26, fy=904.39, principal point cx=689.49, cy=381.59). Made necessary assumptions: AO diameter \u2248 6 meters (visual estimate) and ground plane located at the bottom of the image. Identified the lowest point of the AO from the largest contour, calculated pixel distance to the image bottom, and applied the pinhole camera model to compute real-world distance. Used the formula: Real Height = (Pixel Distance / focal_length) \u00d7 Distance to AO, accounting for perspective projection.\n\n### Task 3: Rotation Period Estimation\n\n**Manual Reference Measurement:**Selected a distinctive feature (easternmost coast of Australia) as a tracking point. Used video editing software to add a vertical reference line on the feature, played the video until the feature completed one full rotation, and measured frame-by-frame for precision. Recorded the manual estimate as the baseline reference.\n\n**Algorithmic Rotation Detection:**Implemented feature-based rotation tracking using SIFT (Scale-Invariant Feature Transform) for keypoint detection. For each video frame:\n\n*   Detected SIFT keypoints and descriptors\n    \n*   Matched features between consecutive frames using brute-force matching with Lowe's ratio test\n    \n*   Computed Essential Matrix from matched keypoints: E = K^T R K\n    \n*   Extracted rotation matrix R from the Essential Matrix using cv2.recoverPose()\n    \n*   Calculated relative rotation: R_relative = R_current \u00d7 R_initial^(-1)\n    \n*   Computed angular displacement: \u03b8 = arccos((tr(R) - 1) / 2)\n    \n*   Accumulated angles over time and detected full rotations when displacement approached 360\u00b0 (\u00b15\u00b0 tolerance)\n    \n\nRecorded multiple rotation periods throughout the video, averaged them to minimize noise effects, and visualized angular displacement over time with histograms of detected periods.\n\n**Multi-Perspective Comparison:**Applied the same algorithm to videos captured from different viewpoints (side view and bottom view) to assess consistency and identify perspective-dependent variations in rotation measurement.\n\nResults\n-------\n\n### Segmentation Performance\n\n**Color Thresholding:** Successfully isolated the AO based on distinctive ocean, cloud, and land colors. However, exhibited sensitivity to lighting variations, shadows, and background elements with similar HSV values. Some noise appeared around the AO due to overlapping color ranges.\n\n**Edge Detection:** Produced clean circular masks with minimal noise, effectively leveraging the AO's spherical geometry. Occasionally failed when the AO appeared oval due to perspective or when multiple circular features confused the detector.\n\n**Combined Method:** Achieved the best overall performance by eliminating most noise while maintaining accurate boundaries. ROC curve analysis showed Methods B and C (edge detection and combined) clustered in the top-left region with high true positive rates and low false positive rates, significantly outperforming Method A (color thresholding alone).\n\n### Geometric Measurements\n\n**Centroid Detection:** Consistently identified the geometric center with high accuracy across the dataset. Even when masks contained minor imperfections, the contour-based moment calculation proved robust and reliable.\n\n**Movement Analysis:** The trajectory plot revealed non-linear centroid displacement, indicating complex swinging motion rather than simple linear movement. The irregular path demonstrated the AO's dynamic behavior influenced by environmental factors and mechanical properties.\n\n**Height Estimation:** Calculated height of **19.68 meters** from the AO's lowest point to the ground. While based on assumptions about the AO's diameter and ground plane position, the result appeared reasonable given the visual context and physical constraints.\n\n### Rotation Period Analysis\n\n**Manual Measurement:** Frame-by-frame analysis yielded a rotation period of **492 seconds** (8 minutes, 12 seconds), serving as the reference baseline.\n\n**Side View Algorithm:** Calculated average rotation period of **431.76 seconds**. The angular displacement graph showed steady accumulation over time with most detected periods clustering near the true value. Some outliers appeared due to feature tracking near the poles or when limited landmasses were visible.\n\n**Bottom View Algorithm:** Produced an average rotation period of **401.94 seconds**. The distribution histogram showed higher frequency of periods around 300 seconds, suggesting perspective effects where points near the poles appeared to rotate faster, slightly decreasing the overall average.\n\nThe algorithmic results came within approximately 60 seconds of the manual reference, demonstrating strong agreement despite computational approximations and noise in feature matching.\n\nKey Findings\n------------\n\n**Segmentation Synergy:** Combining color-based and geometry-based methods significantly improved robustness by compensating for each method's individual weaknesses\u2014color thresholding handled complex patterns while edge detection ensured geometric accuracy.\n\n**Robust Centroid Tracking:** Moment-based calculations on contours proved resilient even with imperfect masks, making this approach suitable for dynamic object tracking applications.\n\n**Perspective Dependency:** Rotation measurements varied between viewpoints, with bottom-view videos producing slightly lower period estimates due to increased weighting of polar regions in the calculation.\n\n**Practical Applicability:** Despite relying on assumptions for height estimation, the computer vision approach produced plausible real-world measurements, demonstrating the viability of single-camera analysis for approximate dimensional assessment.\n\nTechnical Significance\n----------------------\n\nThis project successfully demonstrated the application of classical computer vision techniques to analyze a dynamic three-dimensional object. The integration of multiple segmentation methods, geometric analysis, and feature-based tracking created a comprehensive system capable of extracting meaningful information from visual data. The methodologies developed are directly applicable to robotics applications, particularly drone navigation scenarios requiring object detection, position estimation, and motion prediction. The close agreement between manual and algorithmic rotation measurements validates the approach for real-time video processing applications in autonomous systems.",

    "TechStack": [
      "OpenCV (cv2)",
      "SIFT (Scale-Invariant Feature Transform)",
      "NumPy",
      "Matplotlib",
      "Pyplot",
      "scikit-learn",
      "sklearn.metrics",
      "Arducam OV9782 RGB Camera",
      "w3_calibration.py"
    ],
    "tags": ["Machine Learning", "Deep Learning"],
    "url": "https://drive.google.com/file/d/1wfPhyYVnHJTHjUsPMbp0R9uLfh0Fdx62/view?usp=sharing"
  },
  {
    "title": "Controlling ROBOTNIK for warehouse operations.",

    "description": "Developed an integrated estimation and control system for a wheeled robot (Robotnik model) to navigate to desired positions with known initial configuration. The project combined Extended Kalman Filter (EKF) for state estimation with Model Predictive Control (MPC) for trajectory tracking, creating an optimal autonomous navigation system that handles real-world uncertainties and nonlinearities.\n\nProcess\n-------\n\n### System Components\n\nImplemented the solution using Python with NumPy for mathematical operations, Matplotlib for visualization, PyBullet as the simulation environment, and Pinocchio for robot dynamics calculations. The wheeled robot configuration was defined in a robotnik.json file, representing a differential drive mobile robot.\n\n### Task 1: Extended Kalman Filter Enhancement\n\n**Initial Range-Only Localization:**Started with a baseline EKF system that used only distance measurements to landmarks. This system struggled with accuracy, producing position estimates with standard deviations around 0.5 meters\u2014far exceeding the required 0.1-meter threshold.\n\n**Landmark Grid Expansion:**Initially added 5 manually placed landmarks at positions like [5,10], [15,5], [10,15], [10,10], and [15,15]. When random placement proved insufficient, implemented a systematic 11\u00d711 grid covering the workspace from -20 to 20 in both X and Y directions with 4-unit spacing: [[x, y] for x in range(-20, 21, 4) for y in range(-20, 21, 4)]. This uniform coverage dramatically improved localization accuracy across the entire workspace.\n\n**Range-Bearing Sensor Integration:**Extended the filter to incorporate both range and bearing measurements. Created a new update function update_from_landmark_range_bearing_observations() that computes:\n\n*   Predicted range: sqrt(dx\u00b2 + dy\u00b2)\n    \n*   Predicted bearing: arctan2(dy, dx) - robot_heading\n    \n*   Measurement Jacobian matrix accounting for both distance and angular observations\n    \nFor each landmark, calculated the Jacobian:\n\n```python\nC_range = [-dx/range, -dy/range, 0]\nC_bearing = [dy/range\u00b2, -dx/range\u00b2, -1]\n```\n\nApplied angle wrapping to keep bearing measurements within [-\u03c0, \u03c0] and computed innovations (measurement residuals) for both range and bearing. Combined covariance matrices for range and bearing noise (W_range and W_bearing) into a block-diagonal matrix for the update step.\n\n**Observation Generation:**Implemented a sensor simulation function that generates noisy measurements by adding Gaussian noise to true range and bearing values based on the robot's actual position and landmark locations.\n\n### Task 2: Model Predictive Control Implementation\n\n**Dynamic Linearization:**Linearized the nonlinear robot dynamics around the current state trajectory. For a differential drive robot with dynamics:\n\n*   ẋ = v\u00b7cos(\u03b8)\n    \n*   ẏ = v\u00b7sin(\u03b8)\n    \n*   \u03b8\u0307 = \u03c9\n    \nComputed the Jacobian matrices:\n\n**State Jacobian (Ac):**\n\n```python\nAc = [[0,  0,  -v_0\u00b7sin(\u03b8_0)],\n      [0,  0,   v_0\u00b7cos(\u03b8_0)],\n      [0,  0,   0         ]]\n```\n\n**Control Jacobian (Bc):**\n\n```python\nBc = [[cos(\u03b8_0),  0],\n      [sin(\u03b8_0),  0],\n      [0,            1]]\n```\n\nDiscretized for time step \u0394t:\n\n*   A = I + \u0394t\u00b7Ac\n    \n*   B = \u0394t\u00b7Bc\n    \n**Static Linearization:**Also tested linearization around the goal state (0,0,0) with zero velocity, resulting in:\n\n```python\nA = [[1, 0, 0],\n     [0, 1, 0],\n     [0, 0, 1]]\nB = [[\u0394t,  0 ],\n     [0,     0 ],\n     [0,    \u0394t]]\n```\n\n**Horizon Length Tuning:**Tested prediction horizons (N) of 10, 15, 20, 30, and 60 time steps to balance prediction accuracy against computational cost. Each horizon value affects how far ahead the controller plans its trajectory.\n\n**Cost Function Tuning:**Adjusted state error weights (Q matrix) and control effort weights (R scalar):\n\n*   Initial: Q_coeff = [310, 310, 80]\n    \n*   Refined: Q_coeff = [400, 400, 50]\n    \n*   R_coeff = 0.5 (initial), 2.0 (refined)\n\nIncreased position weights (x, y) to prioritize reaching the target while reducing heading weight and increasing control penalty to prevent aggressive maneuvers.\n\n**Terminal Cost Matrix:**Introduced terminal cost matrix P computed via Discrete Algebraic Riccati Equation (DARE) using scipy.linalg.solve_discrete_are(). Modified the propagation model to use P instead of Q for the final prediction step, providing stronger penalty for terminal state error without increasing computation across all horizon steps.\n\n### Task 3: EKF-MPC Integration\n\n**Full System Integration:**Combined the enhanced EKF with the tuned MPC controller in the main simulation loop:\n\n1.  **Initialization:** Created estimator instance from the range-bearing EKF\n    \n2.  **Control Loop:**\n    \n    *   Set current control input to the estimator\n        \n    *   Predict robot state forward to current time\n        \n    *   Generate noisy range-bearing observations from true position\n        \n    *   Update EKF with observations\n        \n    *   Extract state estimate and covariance\n        \n    *   Feed estimated state (not true state) into MPC\n        \n    *   Compute optimal control action\n        \n    *   Apply control to robot dynamics\n\nThis closed-loop architecture simulates realistic operation where the controller only has access to noisy sensor data filtered through the estimator.\n\nResults\n-------\n\n### EKF Localization Performance\n\n**Range-Only Baseline:** Produced estimated paths that deviated significantly from true trajectories. Standard deviations for X and Y coordinates reached approximately 0.5 meters, five times the required 0.1-meter specification. Without orientation information, multiple robot configurations were consistent with distance measurements alone.\n\n**Random Landmarks:** Adding 5 manually placed landmarks showed marginal improvement. Estimation accuracy increased slightly in regions near landmarks but remained poor overall due to non-uniform coverage.\n\n**Grid Landmarks (Range-Only):** The 11\u00d711 landmark grid dramatically improved performance, bringing standard deviations much closer to the requirement. Uniform spatial coverage enabled consistent localization accuracy across the workspace.\n\n**Range-Bearing (Sparse Landmarks):** Adding bearing measurements with only 5 landmarks produced better path tracking than range-only with 5 landmarks. The orientation information reduced ambiguity in robot configuration, but still failed to meet the 0.1-meter standard deviation requirement.\n\n**Range-Bearing + Grid Landmarks:** Achieved **optimal performance**, successfully meeting the 0.1-meter standard deviation specification for both X and Y coordinates. Estimated trajectories closely matched true paths with minimal deviation. This combination leveraged both orientation information and comprehensive spatial coverage.\n\n### MPC Control Performance\n\n**Dynamic Linearization:** The robot successfully navigated to the goal position with slight overshoot and oscillation before stabilizing. Predicted and actual trajectories showed excellent agreement, nearly overlapping in all plots. Control inputs converged smoothly to zero at the destination.\n\n**Static Linearization:** The robot approached the goal and slowed down appropriately but stopped slightly short of the exact target position. While stable, it lacked the precision of dynamic linearization due to the approximation being valid only near the goal.\n\n**Horizon Length Analysis:**\n\n*   **N = 10:** Fast convergence with good stability\n    \n*   **N = 15-20:** Similar control behavior but slower execution due to increased computation\n    \n*   **N = 30-60:** Significantly longer computation times with minimal improvement in trajectory quality\n\nControl signal shapes remained similar across all horizons, but execution time scaled dramatically with N.\n\n**Q/R Tuning Results:**\n\n*   **Initial weights [310, 310, 80], R=0.5:** System reached target but exhibited oscillations around the goal\n    \n*   **Refined weights [400, 400, 50], R=2.0:** Eliminated oscillations by prioritizing position accuracy over aggressive control, producing smooth, stable convergence to the goal\n\n**Terminal Cost Integration:** Adding the DARE-computed matrix P eliminated oscillations while maintaining fast convergence. Despite some initial transients, the system stabilized quickly at the goal. Control inputs, position coordinates, and heading angle all showed clean convergence without overshoot.\n\n### Integrated System Performance\n\nThe combined EKF-MPC system successfully navigated the robot to desired positions using only noisy sensor measurements. The range-bearing EKF with grid landmarks provided sufficiently accurate state estimates for the MPC to compute effective control actions. The system demonstrated robustness to measurement noise and model uncertainties.\n\nKey Findings\n------------\n\n**Sensor Fusion Superiority:** Combining range and bearing measurements significantly outperformed range-only sensing. Orientation information resolved configuration ambiguities that distance measurements alone could not address.\n\n**Spatial Coverage Importance:** Uniform landmark distribution (grid layout) proved far more effective than random placement. Comprehensive workspace coverage enabled consistent localization accuracy regardless of robot position.\n\n**Dynamic vs Static Linearization:** Dynamic linearization around the current trajectory outperformed static linearization around the goal. The additional computational cost was justified by improved tracking accuracy and guaranteed convergence to the exact target.\n\n**Horizon Length Trade-off:** A prediction horizon of 10 steps provided the best balance between control quality and computational efficiency. Longer horizons offered marginal improvements at substantial computational expense.\n\n**Cost Function Design:** Properly weighted state errors and control penalties are critical for stable, non-oscillatory behavior. Prioritizing position accuracy (high Q for x, y) while moderately penalizing control effort (moderate R) produced optimal results.\n\n**Terminal Cost Benefits:** The DARE-based terminal cost matrix improved stability without requiring longer prediction horizons, effectively reducing computational burden while maintaining or improving performance.\n\nTechnical Significance\n----------------------\n\nThis project successfully demonstrated the integration of modern estimation and control techniques for autonomous mobile robot navigation. The EKF provided robust state estimation under realistic sensor noise conditions, while the MPC enabled predictive, optimal control that explicitly handled system constraints and dynamics. The systematic tuning methodology\u2014from sensor selection and landmark placement to horizon length and cost function weights\u2014provides a reusable framework for similar robotic applications. The results validate that well-designed model-based control systems can achieve precise navigation using only noisy measurements, essential for real-world autonomous systems operating without perfect state information.",
    "TechStack": [
      "NumPy (1.26.4)",
      "Matplotlib (3.9.1)",
      "PyBullet (3.2.5)",
      "Pinocchio (2.7.0)",
      "scipy.linalg",
      "Robotnik (wheeled robot)",
      "robotnik.json",
      "Extended Kalman Filter (EKF)",
      "Model Predictive Control (MPC)",
      "DARE (Discrete Algebraic Riccati Equation)"
    ],
    "url": " https://drive.google.com/file/d/1i4AA2GC4oQ6hYBFpBi_eNcEsQC-Zytw0/view?usp=sharing",
    "tags": ["Robotics"]
  },
  {
    "title": "Drone trajectory planning to visit target points and avoid obstacles",
    "description": "An Aerial Robotics (M.Sc) coursework to implement a path planning algorithm using the Travelling Salesman and A-star algorithms, to navigate all the waypoints, avoiding obstacles.",
    "tags": ["Robotics"],
    "TechStack": [
      "Python",
      "NumPy",
      "Matplotlib",
      "A-star algorithm",
      "Travelling Salesman Problem (TSP)"
    ],
    "url": "https://drive.google.com/file/d/193MBg2Au_FwguZoY5LYxowWJKJ0Or6Dx/view?usp=sharing"
  },
  {
    "title": "Book Recommender",
    "description": "A book recommender built using openai and HuggingFace APIs for vector-search, text classification and sentiment analysis.",
    "TechStack": [
      "Python",
      "OpenAI API",
      "HuggingFace Transformers",
      "scikit-learn",
      "numpy",
      "pandas"
    ],
    "tags": ["Machine Learning", "GenAI"]
  },
  {
    "title": "Meet-up app",
    "description": "This app was created as a hands-on project to learn React and Firebase. It enables users to browse, filter, and add meet-ups, with all data stored in a real-time Firebase database. Please note, this is a simplified meetup app prototype — it lacks user authentication and control features. Its primary purpose is to demonstrate React context usage rather than serve as a fully functional meetup platform.",

    "TechStack": ["React", "Firebase"],
    "tags": ["Frontend Web Development"]
  }
]
